# SOP LLM MMoE - æ··åˆä¸“å®¶æ¨¡å‹èåˆæ–¹æ¡ˆ

åŸºäºQwen3-8Bçš„Foundationæ¨¡å‹å’ŒSFTæ¨¡å‹èåˆé¡¹ç›®ï¼Œé€šè¿‡ä¸‰ä¸ªé˜¶æ®µé€æ­¥å®ç°æ¨¡å‹èåˆå’Œä¼˜åŒ–ã€‚

## é¡¹ç›®èƒŒæ™¯

æœ¬é¡¹ç›®è§£å†³SFTæ¨¡å‹è¿‡æ‹Ÿåˆã€æ³›åŒ–èƒ½åŠ›ä¸è¶³çš„é—®é¢˜ï¼Œé€šè¿‡èåˆFoundationæ¨¡å‹çš„æ¨ç†èƒ½åŠ›å’ŒSFTæ¨¡å‹çš„SOPéµå¾ªèƒ½åŠ›ï¼Œå®ç°æœ€ä½³æ€§èƒ½ã€‚

## é¡¹ç›®ç»“æ„

```
sop_llm_mmoe/
â”œâ”€â”€ configs/
â”‚   â”œâ”€â”€ config.yaml                    # ä¸»é…ç½®æ–‡ä»¶
â”‚   â””â”€â”€ llamafactory_distill.yaml      # LlamaFactoryè’¸é¦é…ç½®
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ foundation/                    # ã€è¯·æ”¾ç½®ã€‘Qwen3-8B Foundationæ¨¡å‹
â”‚   â”œâ”€â”€ sft/                          # ã€è¯·æ”¾ç½®ã€‘Qwen3-8B SFTæ¨¡å‹
â”‚   â””â”€â”€ checkpoints/
â”‚       â”œâ”€â”€ btm/                      # BTM router checkpoints
â”‚       â””â”€â”€ distill/                  # è’¸é¦åçš„studentæ¨¡å‹
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ train/
â”‚   â”‚   â””â”€â”€ train.jsonl               # è®­ç»ƒæ•°æ®
â”‚   â””â”€â”€ eval/
â”‚       â”œâ”€â”€ eval.jsonl                # é€šç”¨è¯„ä¼°æ•°æ®
â”‚       â”œâ”€â”€ sop_strict.jsonl          # SOPä¸¥æ ¼éµå¾ªæµ‹è¯•
â”‚       â”œâ”€â”€ reasoning.jsonl           # æ¨ç†èƒ½åŠ›æµ‹è¯•
â”‚       â””â”€â”€ mixed.jsonl               # æ··åˆåœºæ™¯æµ‹è¯•
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ utils/                        # é€šç”¨å·¥å…·
â”‚   â”œâ”€â”€ step1_ensemble/               # Step 1: ç®€å•Ensemble
â”‚   â”œâ”€â”€ step2_btm/                    # Step 2: BTMè®­ç»ƒ
â”‚   â””â”€â”€ step3_distill/                # Step 3: è’¸é¦è®­ç»ƒ
â”œâ”€â”€ outputs/                          # è¾“å‡ºç»“æœ
â”œâ”€â”€ logs/                             # è®­ç»ƒæ—¥å¿—
â”œâ”€â”€ evaluate.py                       # ç»Ÿä¸€è¯„ä¼°è„šæœ¬
â””â”€â”€ README.md

```

## ç¯å¢ƒå‡†å¤‡

### 1. å®‰è£…ä¾èµ–

```bash
pip install torch transformers accelerate peft tqdm pyyaml
pip install tensorboard  # å¯é€‰ï¼Œç”¨äºå¯è§†åŒ–

# å¦‚æœä½¿ç”¨LlamaFactory
pip install llamafactory-cli
```

### 2. å‡†å¤‡æ¨¡å‹

å°†ä½ çš„æ¨¡å‹æ”¾åˆ°å¯¹åº”ç›®å½•ï¼š

```bash
# Foundationæ¨¡å‹
models/foundation/
  â”œâ”€â”€ config.json
  â”œâ”€â”€ model.safetensors (æˆ–pytorch_model.bin)
  â”œâ”€â”€ tokenizer.json
  â””â”€â”€ ...

# SFTæ¨¡å‹
models/sft/
  â”œâ”€â”€ config.json
  â”œâ”€â”€ model.safetensors
  â”œâ”€â”€ tokenizer.json
  â””â”€â”€ ...
```

### 3. å‡†å¤‡æ•°æ®

æ•°æ®æ ¼å¼ä¸ºJSONLï¼Œæ¯è¡Œä¸€ä¸ªJSONå¯¹è±¡ï¼š

```json
{"input": "ç”¨æˆ·è¾“å…¥", "output": "æœŸæœ›è¾“å‡º"}
```

ç¤ºä¾‹ï¼š

```bash
# åˆ›å»ºç¤ºä¾‹æ•°æ®
python -c "from src.utils.data_utils import create_sample_data; create_sample_data()"
```

## ä½¿ç”¨æµç¨‹

### Step 1: Simple Ensemble (1å¤©)

**ç›®æ ‡**: å¿«é€ŸéªŒè¯ä¸¤ä¸ªæ¨¡å‹èåˆçš„æ€§èƒ½ä¸Šé™

```bash
# æµ‹è¯•Ensemble
python src/step1_ensemble/inference.py \
  --config configs/config.yaml \
  --prompt "è¯·æŒ‰ç…§SOPæµç¨‹å¤„ç†å®¢æˆ·é€€æ¬¾ç”³è¯·"

# æ‰¹é‡è¯„ä¼°
python src/step1_ensemble/inference.py \
  --config configs/config.yaml \
  --eval_file data/eval/eval.jsonl \
  --output_file outputs/ensemble_results.jsonl
```

**å‚æ•°è¯´æ˜**:
- `--foundation_weight`: Foundationæ¨¡å‹æƒé‡ (é»˜è®¤0.5)
- `--sft_weight`: SFTæ¨¡å‹æƒé‡ (é»˜è®¤0.5)
- `--prompt`: å•ä¸ªpromptæµ‹è¯•
- `--eval_file`: æ‰¹é‡è¯„ä¼°æ–‡ä»¶

**é¢„æœŸç»“æœ**:
- éªŒè¯ensembleä¸Šé™æ€§èƒ½
- åˆ†æä¸¤ä¸ªæ¨¡å‹å„è‡ªçš„ä¼˜åŠ¿åœºæ™¯

---

### Step 2: BTMè®­ç»ƒ (3-5å¤©)

**ç›®æ ‡**: è®­ç»ƒrouterï¼Œæ™ºèƒ½èåˆä¸¤ä¸ªæ¨¡å‹

#### 2.1 è®­ç»ƒRouter

```bash
# è®­ç»ƒBTM router
python src/step2_btm/train.py \
  --config configs/config.yaml \
  --output_dir models/checkpoints/btm
```

**é…ç½®å‚æ•°** (åœ¨`configs/config.yaml`ä¸­ä¿®æ”¹):
```yaml
btm:
  num_epochs: 3
  batch_size: 2
  gradient_accumulation_steps: 8
  learning_rate: 1e-4
  router_type: "layer_level"  # token_levelæˆ–layer_level
  load_balance_weight: 0.01
  sparsity_weight: 0.001
```

**è®­ç»ƒç›‘æ§**:
```bash
# æŸ¥çœ‹æ—¥å¿—
tail -f logs/btm_training.log

# TensorBoard (å¯é€‰)
tensorboard --logdir outputs/tensorboard
```

#### 2.2 BTMæ¨ç†

```bash
# ä½¿ç”¨è®­ç»ƒå¥½çš„routeræ¨ç†
python src/step2_btm/inference.py \
  --config configs/config.yaml \
  --router_checkpoint models/checkpoints/btm/best_router.pt \
  --prompt "è§£é‡Šé‡å­çº ç¼ ç°è±¡" \
  --show_router_stats

# æ‰¹é‡è¯„ä¼°
python src/step2_btm/inference.py \
  --config configs/config.yaml \
  --router_checkpoint models/checkpoints/btm/best_router.pt \
  --eval_file data/eval/eval.jsonl \
  --output_file outputs/btm_results.jsonl \
  --show_router_stats
```

**Routerç»Ÿè®¡**:
æ˜¾ç¤ºæ¯ä¸ªqueryä½¿ç”¨Foundation vs SFTçš„æ¯”ä¾‹ï¼Œå¸®åŠ©ç†è§£routingç­–ç•¥ã€‚

---

### Step 3: è’¸é¦è®­ç»ƒ (1-2å‘¨)

**ç›®æ ‡**: å°†BTMèƒ½åŠ›è’¸é¦åˆ°å•ä¸€æ¨¡å‹ï¼Œé™ä½æ¨ç†æˆæœ¬

#### 3.1 ä½¿ç”¨è‡ªå®šä¹‰trainerè’¸é¦

```bash
# è’¸é¦è®­ç»ƒ
python src/step3_distill/train.py \
  --config configs/config.yaml \
  --teacher_type btm \
  --btm_router models/checkpoints/btm/best_router.pt \
  --student_init foundation \
  --output_dir models/checkpoints/distill
```

**å‚æ•°è¯´æ˜**:
- `--teacher_type`: Teacherç±»å‹ (btmæˆ–ensemble)
- `--btm_router`: BTM router checkpointè·¯å¾„
- `--student_init`: Studentåˆå§‹åŒ– (foundation/sft/è‡ªå®šä¹‰è·¯å¾„)

**é…ç½®å‚æ•°**:
```yaml
distill:
  temperature: 2.0                # è’¸é¦æ¸©åº¦
  distill_loss_weight: 0.7       # è’¸é¦lossæƒé‡
  task_loss_weight: 0.3          # ä»»åŠ¡lossæƒé‡
  learning_rate: 5e-6
  num_epochs: 2
  student_init: "foundation"
```

#### 3.2 ä½¿ç”¨LlamaFactoryè’¸é¦ (å¯é€‰)

å¦‚æœä½ ç†Ÿæ‚‰LlamaFactoryï¼Œå¯ä»¥ç”¨å®ƒåšbaseline SFTï¼š

```bash
# 1. å‡†å¤‡æ•°æ®é›†é…ç½® (data/dataset_info.json)
{
  "distill_train": {
    "file_name": "train/train.jsonl",
    "columns": {
      "prompt": "input",
      "response": "output"
    }
  }
}

# 2. è¿è¡ŒLlamaFactoryè®­ç»ƒ
llamafactory-cli train configs/llamafactory_distill.yaml
```

---

## è¯„ä¼°å¯¹æ¯”

ç»Ÿä¸€è¯„ä¼°æ‰€æœ‰æ¨¡å‹ï¼š

```bash
python evaluate.py \
  --config configs/config.yaml \
  --models foundation sft btm distill \
  --btm_router models/checkpoints/btm/best_router.pt \
  --distill_model models/checkpoints/distill/best_student \
  --output_file outputs/evaluation_results.json
```

**è¯„ä¼°æŒ‡æ ‡**:
- Perplexity: å›°æƒ‘åº¦
- Accuracy: å‡†ç¡®ç‡
- åˆ†ç±»è¯„ä¼°: SOPéµå¾ª vs æ¨ç†èƒ½åŠ› vs æ··åˆåœºæ™¯

**è¾“å‡ºç¤ºä¾‹**:
```
è¯„ä¼°ç»“æœå¯¹æ¯”
================================================================================

sop_strict:
æ¨¡å‹             Perplexity      Accuracy
--------------------------------------------------
foundation      25.30           65.00%
sft             18.50           95.00%
btm             19.20           92.00%
distill         20.10           90.00%

reasoning:
æ¨¡å‹             Perplexity      Accuracy
--------------------------------------------------
foundation      22.10           88.00%
sft             28.40           70.00%
btm             23.50           86.00%
distill         24.20           85.00%
```

---

## é…ç½®è¯´æ˜

### ä¸»é…ç½®æ–‡ä»¶: `configs/config.yaml`

å…³é”®é…ç½®é¡¹ï¼š

```yaml
# æ¨¡å‹è·¯å¾„
models:
  foundation:
    path: "./models/foundation"
  sft:
    path: "./models/sft"

# æ•°æ®
data:
  train_file: "./data/train/train.jsonl"
  eval_file: "./data/eval/eval.jsonl"

# Ensembleæƒé‡
ensemble:
  foundation_weight: 0.5
  sft_weight: 0.5

# BTM
btm:
  router_type: "layer_level"
  learning_rate: 1e-4
  num_epochs: 3

# Distillation
distill:
  temperature: 2.0
  distill_loss_weight: 0.7
  task_loss_weight: 0.3
  learning_rate: 5e-6
  num_epochs: 2
```

---

## æ˜¾å­˜éœ€æ±‚

| é˜¶æ®µ | è®­ç»ƒæ˜¾å­˜ | æ¨ç†æ˜¾å­˜ | å¤‡æ³¨ |
|------|---------|---------|------|
| Step 1 Ensemble | 0 | 2Ã—16GB = 32GB | å¹¶è¡ŒåŠ è½½ä¸¤ä¸ªæ¨¡å‹ |
| Step 2 BTMè®­ç»ƒ | 2Ã—16GB + 2GB | 2Ã—16GB | Routerå¾ˆå° |
| Step 2 BTMæ¨ç† | 0 | 2Ã—16GB | å¹¶è¡Œæ¨ç† |
| Step 3 è’¸é¦è®­ç»ƒ | 2Ã—16GB + 16GB | 16GB | 2ä¸ªteacher + 1ä¸ªstudent |
| Step 3 è’¸é¦æ¨ç† | 0 | 16GB | å•æ¨¡å‹æ¨ç† |

**ä¼˜åŒ–å»ºè®®**:
- ä½¿ç”¨`bf16`æˆ–`fp16`å‡å°‘æ˜¾å­˜
- ä½¿ç”¨`gradient_checkpointing`
- ä½¿ç”¨`load_in_8bit=True`é‡åŒ–
- å¤šGPUåˆ†å¸ƒå¼è®­ç»ƒ

---

## å¸¸è§é—®é¢˜

### Q1: æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨

```bash
# æ£€æŸ¥é…ç½®
python src/utils/config_loader.py
```

### Q2: æ˜¾å­˜ä¸è¶³

æ–¹æ¡ˆï¼š
1. å‡å°batch_size
2. å¢åŠ gradient_accumulation_steps
3. ä½¿ç”¨8bité‡åŒ–
4. ä½¿ç”¨æ›´å°çš„æ¨¡å‹æµ‹è¯•æµç¨‹

### Q3: BTMè®­ç»ƒlossä¸ä¸‹é™

æ£€æŸ¥ï¼š
1. æ•°æ®è´¨é‡æ˜¯å¦è¶³å¤Ÿå¥½
2. learning_rateæ˜¯å¦åˆé€‚ (å°è¯•1e-5åˆ°1e-3)
3. load_balance_weightæ˜¯å¦è¿‡å¤§ (å¯¼è‡´routerä¸å­¦ä¹ )

### Q4: è’¸é¦åæ€§èƒ½ä¸‹é™æ˜æ˜¾

å»ºè®®ï¼š
1. å¢åŠ è®­ç»ƒæ•°æ®é‡
2. è°ƒæ•´temperature (1.5-3.0)
3. è°ƒæ•´lossæƒé‡æ¯”ä¾‹
4. ä½¿ç”¨æ›´å¥½çš„studentåˆå§‹åŒ–

---

## è¿›é˜¶ä½¿ç”¨

### 1. è‡ªå®šä¹‰Routeræ¶æ„

ç¼–è¾‘`src/step2_btm/model.py`ï¼Œä¿®æ”¹`TokenRouter`ç±»ï¼š

```python
class TokenRouter(nn.Module):
    def __init__(self, ...):
        # è‡ªå®šä¹‰ç½‘ç»œç»“æ„
        self.router_net = nn.Sequential(
            # ä½ çš„æ¶æ„
        )
```

### 2. æ·»åŠ æ–°çš„è¯„ä¼°æŒ‡æ ‡

ç¼–è¾‘`evaluate.py`ï¼Œåœ¨`ModelEvaluator`ç±»ä¸­æ·»åŠ æ–°æ–¹æ³•ï¼š

```python
def evaluate_custom_metric(self, ...):
    # å®ç°ä½ çš„æŒ‡æ ‡
    pass
```

### 3. Layer-wiseä¸åŒç­–ç•¥

ä¿®æ”¹BTMé…ç½®ï¼Œä¸ºä¸åŒå±‚è®¾ç½®ä¸åŒçš„routingç­–ç•¥ï¼ˆéœ€è¦ä¿®æ”¹ä»£ç ï¼‰ã€‚

---

## æ€§èƒ½å¯¹æ¯”å‚è€ƒ

åŸºäºQwen3-8Bçš„é¢„æœŸæ€§èƒ½ï¼ˆä»…ä¾›å‚è€ƒï¼‰ï¼š

| æ¨¡å‹ | SOPå‡†ç¡®ç‡ | æ¨ç†èƒ½åŠ› | æ¨ç†æˆæœ¬ | è®­ç»ƒæˆæœ¬ |
|------|---------|---------|---------|---------|
| Foundation | 65% | 88% | 1x | 0 |
| SFT | 95% | 70% | 1x | 0 |
| Ensemble | 96% | 90% | 2x | 0 |
| BTM | 92% | 86% | 2x | ä½ |
| Distill | 90% | 85% | 1x | é«˜ |

**æ¨èè·¯å¾„**:
1. å¿«é€ŸéªŒè¯ â†’ Ensemble
2. ç”Ÿäº§éƒ¨ç½²ï¼ˆæˆæœ¬ä¸æ•æ„Ÿï¼‰â†’ BTM
3. ç”Ÿäº§éƒ¨ç½²ï¼ˆæˆæœ¬æ•æ„Ÿï¼‰â†’ Distill

---

## ä¸‹ä¸€æ­¥

1. **æ•°æ®å‡†å¤‡**: å‡†å¤‡é«˜è´¨é‡çš„çœŸå®caseæ•°æ®ï¼ˆè‡³å°‘1Kæ¡ï¼‰
2. **Step 1éªŒè¯**: è¿è¡ŒEnsembleï¼Œç¡®è®¤èåˆæ•ˆæœ
3. **Step 2è®­ç»ƒ**: è®­ç»ƒBTM routerï¼ˆ2-3å¤©ï¼‰
4. **Step 3è’¸é¦**: å¦‚æœBTMæ•ˆæœå¥½ï¼Œåšè’¸é¦ï¼ˆ1-2å‘¨ï¼‰
5. **ç”Ÿäº§éƒ¨ç½²**: æ ¹æ®æˆæœ¬é€‰æ‹©BTMæˆ–Distilléƒ¨ç½²

---

## å¼•ç”¨

å¦‚æœä½¿ç”¨äº†æœ¬é¡¹ç›®ï¼Œè¯·å¼•ç”¨ç›¸å…³è®ºæ–‡ï¼š

- Branch-Train-Merge: Li et al., 2022
- Model Distillation: Hinton et al., 2015
- Task Arithmetic: Ilharco et al., 2023

---

## è”ç³»

å¦‚æœ‰é—®é¢˜ï¼Œè¯·æŸ¥çœ‹`logs/`ç›®å½•ä¸‹çš„æ—¥å¿—ï¼Œæˆ–æissueã€‚

**Good Luck!** ğŸš€
